{"status":"Failed","policy":{"name":"Webhooks","description":""},"findingKey":"b2CL8c3TEw7DwXiKHZbjeg","findingId":"b2CL8c3TEw7DwXiKHZbjeg","cloudGuardAccountId":"51722","origin":"Compliance Engine","bundle":{"name":"CIS Kubernetes Benchmark v1.6.1","description":"Automated Validation of Kubernetes CIS Benchmark v1.6.1 Prescriptive guidance for establishing a secure configuration posture for Kubernetes 1.6.1\nFor additional reference: https://www.cisecurity.org/benchmark/kubernetes/","id":-70},"reportTime":"2023-06-09T11:00:31.243Z","rule":{"name":"Ensure that the seccomp profile is set to docker/default in your pod definitions","ruleId":"D9.K8S.IAM.21","description":"Seccomp (secure computing mode) is used to restrict the set of system calls applications can make, allowing cluster administrators greater control over the security of workloads running in the cluster. Kubernetes disables seccomp profiles by default for historical reasons. You should enable it to ensure that the workloads have restricted actions available within the container.","remediation":"Use security context to enable the docker/default seccomp profile in your pod definitions.\n\n**References**\n1. https://github.com/kubernetes/kubernetes/issues/39845\n2. https://github.com/kubernetes/kubernetes/pull/21790\n3. https://github.com/kubernetes/community/blob/master/contributors/design-proposals/seccomp.md#examples\n4. https://docs.docker.com/engine/security/seccomp/","complianceTags":"5.6.2","logicHash":"wat89y5RGQIVJriupbowlQ","severity":"High"},"account":{"id":"5af13ab5-72ed-43b6-9544-33c9236d62f0","name":"nu-openshift-cluster","vendor":"Kubernetes","dome9CloudAccountId":"5af13ab5-72ed-43b6-9544-33c9236d62f0","organizationalUnitId":"00000000-0000-0000-0000-000000000000","organizationalUnitPath":""},"region":"Global","entity":{"spec":{"affinity":null,"containers":[{"args":["LOCK=/var/log/kube-apiserver/.lock\n# We should be able to acquire the lock immediatelly. If not, it means the init container has not released it yet and kubelet or CRI-O started container prematurely.\nexec {LOCK_FD}>${LOCK} && flock --verbose -w 30 \"${LOCK_FD}\" || {\n  echo \"Failed to acquire lock for kube-apiserver. Please check setup container for details. This is likely kubelet or CRI-O bug.\"\n  exit 1\n}\nif [ -f /etc/kubernetes/static-pod-certs/configmaps/trusted-ca-bundle/ca-bundle.crt ]; then\n  echo \"Copying system trust bundle ...\"\n  cp -f /etc/kubernetes/static-pod-certs/configmaps/trusted-ca-bundle/ca-bundle.crt /etc/pki/ca-trust/extracted/pem/tls-ca-bundle.pem\nfi\n\nexec watch-termination --termination-touch-file=/var/log/kube-apiserver/.terminating --termination-log-file=/var/log/kube-apiserver/termination.log --graceful-termination-duration=135s --kubeconfig=/etc/kubernetes/static-pod-resources/configmaps/kube-apiserver-cert-syncer-kubeconfig/kubeconfig -- hyperkube kube-apiserver --openshift-config=/etc/kubernetes/static-pod-resources/configmaps/config/config.yaml --advertise-address=${HOST_IP}  -v=2 --permit-address-sharing\n"],"command":["/bin/bash","-ec"],"env":[{"name":"POD_NAME","value":null,"valueFrom":{"secretKeyRef":null}},{"name":"POD_NAMESPACE","value":null,"valueFrom":{"secretKeyRef":null}},{"name":"STATIC_POD_VERSION","value":"57","valueFrom":null},{"name":"HOST_IP","value":null,"valueFrom":{"secretKeyRef":null}},{"name":"GOGC","value":"100","valueFrom":null}],"image":"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:7dd9070ddd5c6d877e6431af587973c39f97f753f2c504b43c4841d5ac3f3733","livenessProbe":{"exec":null,"failureThreshold":3,"httpGet":{"host":null,"path":"livez","port":"6443","scheme":"HTTPS","httpHeaders":null},"initialDelaySeconds":45,"periodSeconds":10,"successThreshold":1,"tcpSocket":null,"timeoutSeconds":10},"name":"kube-apiserver","imageDetails":{"registry":"quay.io/openshift-release-dev","repository":"ocp-v4.0-art-dev","tag":"sha256:7dd9070ddd5c6d877e6431af587973c39f97f753f2c504b43c4841d5ac3f3733"},"parsedArgs":[{"key":"ec","value":"LOCK=/var/log/kube-apiserver/.lock# We should be able to acquire the lock immediatelly. If not, it means the init container has not released it yet and kubelet or CRI-O started container prematurely.exec {LOCK_FD}>${LOCK} && flock"},{"key":"w","value":"30 \"${LOCK_FD}\" || {  echo \"Failed to acquire lock for kube-apiserver. Please check setup container for details. This is likely kubelet or CRI-O bug.\"  exit 1}if ["},{"key":"f","value":"/etc/kubernetes/static-pod-certs/configmaps/trusted-ca-bundle/ca-bundle.crt ]; then  echo \"Copying system trust bundle ...\"  cp"},{"key":"f","value":"/etc/kubernetes/static-pod-certs/configmaps/trusted-ca-bundle/ca-bundle.crt /etc/pki/ca-trust/extracted/pem/tls-ca-bundle.pemfiexec watch-termination"},{"key":"termination-touch-file","value":"/var/log/kube-apiserver/.terminating"},{"key":"termination-log-file","value":"/var/log/kube-apiserver/termination.log"},{"key":"graceful-termination-duration","value":"135s"},{"key":"kubeconfig","value":"/etc/kubernetes/static-pod-resources/configmaps/kube-apiserver-cert-syncer-kubeconfig/kubeconfig"},{"key":"","value":"hyperkube kube-apiserver"},{"key":"openshift-config","value":"/etc/kubernetes/static-pod-resources/configmaps/config/config.yaml"},{"key":"advertise-address","value":"${HOST_IP} "},{"key":"v","value":"2"}],"ports":[{"containerPort":6443,"hostIP":null,"hostPort":6443,"name":null,"protocol":"TCP"}],"readinessProbe":{"exec":null,"failureThreshold":3,"httpGet":{"host":null,"path":"readyz","port":"6443","scheme":"HTTPS","httpHeaders":null},"initialDelaySeconds":10,"periodSeconds":10,"successThreshold":1,"tcpSocket":null,"timeoutSeconds":10},"resources":{"limits":null,"requests":{"memory":"1Gi","cpu":"265m"}},"securityContext":{"allowPrivilegeEscalation":null,"capabilities":null,"privileged":true,"procMount":null,"readOnlyRootFilesystem":null,"runAsGroup":null,"runAsNonRoot":null,"runAsUser":null,"seLinuxOptions":null,"windowsOptions":null,"seccompProfile":null},"volumeMounts":[{"name":"resource-dir","mountPath":"/etc/kubernetes/static-pod-resources","readOnly":false,"mountPropagation":null,"subPath":null,"subPathExpr":null},{"name":"cert-dir","mountPath":"/etc/kubernetes/static-pod-certs","readOnly":false,"mountPropagation":null,"subPath":null,"subPathExpr":null},{"name":"audit-dir","mountPath":"/var/log/kube-apiserver","readOnly":false,"mountPropagation":null,"subPath":null,"subPathExpr":null}]},{"args":["--kubeconfig=/etc/kubernetes/static-pod-resources/configmaps/kube-apiserver-cert-syncer-kubeconfig/kubeconfig","--namespace=$(POD_NAMESPACE)","--destination-dir=/etc/kubernetes/static-pod-certs"],"command":["cluster-kube-apiserver-operator","cert-syncer"],"env":[{"name":"POD_NAME","value":null,"valueFrom":{"secretKeyRef":null}},{"name":"POD_NAMESPACE","value":null,"valueFrom":{"secretKeyRef":null}}],"image":"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3ba4b4f00df0329a9a84a848b4289f2cb2ee3aed069de92f7fae4f0928b35082","livenessProbe":null,"name":"kube-apiserver-cert-syncer","imageDetails":{"registry":"quay.io/openshift-release-dev","repository":"ocp-v4.0-art-dev","tag":"sha256:3ba4b4f00df0329a9a84a848b4289f2cb2ee3aed069de92f7fae4f0928b35082"},"parsedArgs":[{"key":"kube-apiserver-operator","value":"cert-syncer"},{"key":"kubeconfig","value":"/etc/kubernetes/static-pod-resources/configmaps/kube-apiserver-cert-syncer-kubeconfig/kubeconfig"},{"key":"namespace","value":"$(POD_NAMESPACE)"},{"key":"destination-dir","value":"/etc/kubernetes/static-pod-certs"}],"ports":null,"readinessProbe":null,"resources":{"limits":null,"requests":{"memory":"50Mi","cpu":"5m"}},"securityContext":null,"volumeMounts":[{"name":"resource-dir","mountPath":"/etc/kubernetes/static-pod-resources","readOnly":false,"mountPropagation":null,"subPath":null,"subPathExpr":null},{"name":"cert-dir","mountPath":"/etc/kubernetes/static-pod-certs","readOnly":false,"mountPropagation":null,"subPath":null,"subPathExpr":null}]},{"args":["--kubeconfig=/etc/kubernetes/static-pod-resources/configmaps/kube-apiserver-cert-syncer-kubeconfig/kubeconfig","--namespace=$(POD_NAMESPACE)","-v=2"],"command":["cluster-kube-apiserver-operator","cert-regeneration-controller"],"env":[{"name":"POD_NAMESPACE","value":null,"valueFrom":{"secretKeyRef":null}}],"image":"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3ba4b4f00df0329a9a84a848b4289f2cb2ee3aed069de92f7fae4f0928b35082","livenessProbe":null,"name":"kube-apiserver-cert-regeneration-controller","imageDetails":{"registry":"quay.io/openshift-release-dev","repository":"ocp-v4.0-art-dev","tag":"sha256:3ba4b4f00df0329a9a84a848b4289f2cb2ee3aed069de92f7fae4f0928b35082"},"parsedArgs":[{"key":"kube-apiserver-operator","value":"cert-regeneration-controller"},{"key":"kubeconfig","value":"/etc/kubernetes/static-pod-resources/configmaps/kube-apiserver-cert-syncer-kubeconfig/kubeconfig"},{"key":"namespace","value":"$(POD_NAMESPACE)"},{"key":"v","value":"2"}],"ports":null,"readinessProbe":null,"resources":{"limits":null,"requests":{"memory":"50Mi","cpu":"5m"}},"securityContext":null,"volumeMounts":[{"name":"resource-dir","mountPath":"/etc/kubernetes/static-pod-resources","readOnly":false,"mountPropagation":null,"subPath":null,"subPathExpr":null}]},{"args":["--insecure-port=6080","--delegate-url=https://localhost:6443/readyz"],"command":["cluster-kube-apiserver-operator","insecure-readyz"],"env":null,"image":"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3ba4b4f00df0329a9a84a848b4289f2cb2ee3aed069de92f7fae4f0928b35082","livenessProbe":null,"name":"kube-apiserver-insecure-readyz","imageDetails":{"registry":"quay.io/openshift-release-dev","repository":"ocp-v4.0-art-dev","tag":"sha256:3ba4b4f00df0329a9a84a848b4289f2cb2ee3aed069de92f7fae4f0928b35082"},"parsedArgs":[{"key":"kube-apiserver-operator","value":"insecure-readyz"},{"key":"insecure-port","value":"6080"},{"key":"delegate-url","value":"https://localhost:6443/readyz"}],"ports":[{"containerPort":6080,"hostIP":null,"hostPort":6080,"name":null,"protocol":"TCP"}],"readinessProbe":null,"resources":{"limits":null,"requests":{"memory":"50Mi","cpu":"5m"}},"securityContext":null,"volumeMounts":null},{"args":["--kubeconfig","/etc/kubernetes/static-pod-certs/configmaps/check-endpoints-kubeconfig/kubeconfig","--listen","0.0.0.0:17697","--namespace","$(POD_NAMESPACE)","--v","2"],"command":["cluster-kube-apiserver-operator","check-endpoints"],"env":[{"name":"POD_NAME","value":null,"valueFrom":{"secretKeyRef":null}},{"name":"POD_NAMESPACE","value":null,"valueFrom":{"secretKeyRef":null}}],"image":"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3ba4b4f00df0329a9a84a848b4289f2cb2ee3aed069de92f7fae4f0928b35082","livenessProbe":{"exec":null,"failureThreshold":3,"httpGet":{"host":null,"path":"healthz","port":"17697","scheme":"HTTPS","httpHeaders":null},"initialDelaySeconds":10,"periodSeconds":10,"successThreshold":1,"tcpSocket":null,"timeoutSeconds":10},"name":"kube-apiserver-check-endpoints","imageDetails":{"registry":"quay.io/openshift-release-dev","repository":"ocp-v4.0-art-dev","tag":"sha256:3ba4b4f00df0329a9a84a848b4289f2cb2ee3aed069de92f7fae4f0928b35082"},"parsedArgs":[{"key":"kube-apiserver-operator","value":"check-endpoints"},{"key":"kubeconfig","value":"/etc/kubernetes/static-pod-certs/configmaps/check-endpoints-kubeconfig/kubeconfig"},{"key":"listen","value":"0.0.0.0:17697"},{"key":"namespace","value":"$(POD_NAMESPACE)"},{"key":"v","value":"2"}],"ports":[{"containerPort":17697,"hostIP":null,"hostPort":17697,"name":"check-endpoints","protocol":"TCP"}],"readinessProbe":{"exec":null,"failureThreshold":3,"httpGet":{"host":null,"path":"healthz","port":"17697","scheme":"HTTPS","httpHeaders":null},"initialDelaySeconds":10,"periodSeconds":10,"successThreshold":1,"tcpSocket":null,"timeoutSeconds":10},"resources":{"limits":null,"requests":{"memory":"50Mi","cpu":"10m"}},"securityContext":null,"volumeMounts":[{"name":"resource-dir","mountPath":"/etc/kubernetes/static-pod-resources","readOnly":false,"mountPropagation":null,"subPath":null,"subPathExpr":null},{"name":"cert-dir","mountPath":"/etc/kubernetes/static-pod-certs","readOnly":false,"mountPropagation":null,"subPath":null,"subPathExpr":null}]}],"dnsConfig":null,"hostAliases":null,"hostIPC":false,"hostNetwork":true,"hostPID":false,"initContainers":[{"args":["echo \"Fixing audit permissions ...\"\nchmod 0700 /var/log/kube-apiserver && touch /var/log/kube-apiserver/audit.log && chmod 0600 /var/log/kube-apiserver/*\n\nLOCK=/var/log/kube-apiserver/.lock\necho \"Acquiring exclusive lock ${LOCK} ...\"\n\n# Waiting for 135s max for old kube-apiserver's watch-termination process to exit and remove the lock.\n# Two cases:\n# 1. if kubelet does not start the old and new in parallel (i.e. works as expected), the flock will always succeed without any time.\n# 2. if kubelet does overlap old and new pods for up to 130s, the flock will wait and immediate return when the old finishes.\n#\n# NOTE: We can increase 135s for a bigger expected overlap. But a higher value means less noise about the broken kubelet behaviour, i.e. we hide a bug.\n# NOTE: Do not tweak these timings without considering the livenessProbe initialDelaySeconds\nexec {LOCK_FD}>${LOCK} && flock --verbose -w 135 \"${LOCK_FD}\" || {\n  echo \"$(date -Iseconds -u) kubelet did not terminate old kube-apiserver before new one\" >> /var/log/kube-apiserver/lock.log\n  echo -n \": WARNING: kubelet did not terminate old kube-apiserver before new one.\"\n\n  # We failed to acquire exclusive lock, which means there is old kube-apiserver running in system.\n  # Since we utilize SO_REUSEPORT, we need to make sure the old kube-apiserver stopped listening.\n  #\n  # NOTE: This is a fallback for broken kubelet, if you observe this please report a bug.\n  echo -n \"Waiting for port 6443 to be released due to likely bug in kubelet or CRI-O \"\n  while [ -n \"$(ss -Htan state listening '( sport = 6443 or sport = 6080 )')\" ]; do\n    echo -n \".\"\n    sleep 1\n    (( tries += 1 ))\n    if [[ \"${tries}\" -gt 10 ]]; then\n      echo \"Timed out waiting for port :6443 and :6080 to be released, this is likely a bug in kubelet or CRI-O\"\n      exit 1\n    fi\n  done\n  #  This is to make sure the server has terminated independently from the lock.\n  #  After the port has been freed (requests can be pending and need 60s max).\n  sleep 65\n}\n# We cannot hold the lock from the init container to the main container. We release it here. There is no risk, at this point we know we are safe.\nflock -u \"${LOCK_FD}\"\n"],"command":["/usr/bin/timeout","220","/bin/bash","-ec"],"env":null,"image":"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:7dd9070ddd5c6d877e6431af587973c39f97f753f2c504b43c4841d5ac3f3733","livenessProbe":null,"name":"setup","imageDetails":{"registry":"quay.io/openshift-release-dev","repository":"ocp-v4.0-art-dev","tag":"sha256:7dd9070ddd5c6d877e6431af587973c39f97f753f2c504b43c4841d5ac3f3733"},"parsedArgs":[{"key":"ec","value":"echo \"Fixing audit permissions ...\"chmod 0700 /var/log/kube-apiserver && touch /var/log/kube-apiserver/audit.log && chmod 0600 /var/log/kube-apiserver/*LOCK=/var/log/kube-apiserver/.lockecho \"Acquiring exclusive lock ${LOCK} ...\"# Waiting for 135s max for old kube-apiserver's watch-termination process to exit and remove the lock.# Two cases:# 1. if kubelet does not start the old and new in parallel (i.e. works as expected), the flock will always succeed without any time.# 2. if kubelet does overlap old and new pods for up to 130s, the flock will wait and immediate return when the old finishes.## NOTE: We can increase 135s for a bigger expected overlap. But a higher value means less noise about the broken kubelet behaviour, i.e. we hide a bug.# NOTE: Do not tweak these timings without considering the livenessProbe initialDelaySecondsexec {LOCK_FD}>${LOCK} && flock"},{"key":"w","value":"135 \"${LOCK_FD}\" || {  echo \"$(date"},{"key":"u)","value":"kubelet did not terminate old kube-apiserver before new one\" >> /var/log/kube-apiserver/lock.log  echo"},{"key":"n","value":"\": WARNING: kubelet did not terminate old kube-apiserver before new one.\"  # We failed to acquire exclusive lock, which means there is old kube-apiserver running in system.  # Since we utilize SO_REUSEPORT, we need to make sure the old kube-apiserver stopped listening.  #  # NOTE: This is a fallback for broken kubelet, if you observe this please report a bug.  echo"},{"key":"n","value":"\"Waiting for port 6443 to be released due to likely bug in kubelet or CRI-O \"  while ["},{"key":"n","value":"\"$(ss"},{"key":"Htan","value":"state listening '( sport = 6443 or sport = 6080 )')\" ]; do    echo"},{"key":"n","value":"\".\"    sleep 1    (( tries += 1 ))    if [[ \"${tries}\""},{"key":"gt","value":"10 ]]; then      echo \"Timed out waiting for port :6443 and :6080 to be released, this is likely a bug in kubelet or CRI-O\"      exit 1    fi  done  #  This is to make sure the server has terminated independently from the lock.  #  After the port has been freed (requests can be pending and need 60s max).  sleep 65}# We cannot hold the lock from the init container to the main container. We release it here. There is no risk, at this point we know we are safe.flock"},{"key":"u","value":"\"${LOCK_FD}\""}],"ports":null,"readinessProbe":null,"resources":{"limits":null,"requests":{"memory":"50Mi","cpu":"5m"}},"securityContext":{"allowPrivilegeEscalation":null,"capabilities":null,"privileged":true,"procMount":null,"readOnlyRootFilesystem":null,"runAsGroup":null,"runAsNonRoot":null,"runAsUser":null,"seLinuxOptions":null,"windowsOptions":null,"seccompProfile":null},"volumeMounts":[{"name":"audit-dir","mountPath":"/var/log/kube-apiserver","readOnly":false,"mountPropagation":null,"subPath":null,"subPathExpr":null}]}],"nodeInfo":{"conditions":[{"message":"kubelet has sufficient memory available","reason":"KubeletHasSufficientMemory","status":"False","type":"MemoryPressure"},{"message":"kubelet has no disk pressure","reason":"KubeletHasNoDiskPressure","status":"False","type":"DiskPressure"},{"message":"kubelet has sufficient PID available","reason":"KubeletHasSufficientPID","status":"False","type":"PIDPressure"},{"message":"kubelet is posting ready status","reason":"KubeletReady","status":"True","type":"Ready"}],"labels":[{"key":"beta.kubernetes.io/arch","value":"amd64"},{"key":"beta.kubernetes.io/os","value":"linux"},{"key":"kubernetes.io/arch","value":"amd64"},{"key":"kubernetes.io/hostname","value":"control-2.demoenv.local"},{"key":"kubernetes.io/os","value":"linux"},{"key":"node-role.kubernetes.io/master","value":""},{"key":"node.openshift.io/os_id","value":"rhcos"}]},"nodeName":"control-2.demoenv.local","priority":2000001000,"priorityClassName":"system-node-critical","securityContext":{"fsGroup":null,"runAsGroup":null,"runAsNonRoot":null,"runAsUser":null,"seLinuxOptions":null,"supplementalGroups":null,"sysctls":null,"windowsOptions":null,"seccompProfile":null},"serviceAccount":null,"serviceAccountName":null,"tolerations":[{"key":null,"operator":"Exists","effect":null}],"volumes":[{"hostPath":{"path":"/etc/kubernetes/static-pod-resources/kube-apiserver-pod-57","type":""},"flexVolume":null,"name":"resource-dir","persistentVolumeClaim":null},{"hostPath":{"path":"/etc/kubernetes/static-pod-resources/kube-apiserver-certs","type":""},"flexVolume":null,"name":"cert-dir","persistentVolumeClaim":null},{"hostPath":{"path":"/var/log/kube-apiserver","type":""},"flexVolume":null,"name":"audit-dir","persistentVolumeClaim":null}]},"status":{"phase":"Running","podIP":"192.168.10.12"},"networkPolicies":{"ingress":[],"egress":[]},"owner":{"ownerReferences":[{"kind":"Node","uid":"9c10e98c-a08c-4eb7-a532-753ac82d344d","name":"control-2.demoenv.local"}],"rootOwner":{"kind":"Node","uid":"9c10e98c-a08c-4eb7-a532-753ac82d344d","name":"control-2.demoenv.local"}},"tags":[{"key":"apiserver","value":"true"},{"key":"app","value":"openshift-kube-apiserver"},{"key":"revision","value":"57"}],"namespace":"openshift-kube-apiserver","annotations":[{"key":"kubectl.kubernetes.io/default-container","value":"kube-apiserver"},{"key":"kubernetes.io/config.hash","value":"711670017b0ee5659660d7a06f26a022"},{"key":"kubernetes.io/config.mirror","value":"711670017b0ee5659660d7a06f26a022"},{"key":"kubernetes.io/config.seen","value":"2023-05-30T15:13:54.715244055Z"},{"key":"kubernetes.io/config.source","value":"file"},{"key":"target.workload.openshift.io/management","value":"{\"effect\": \"PreferredDuringScheduling\"}"}],"labels":[{"key":"apiserver","value":"true"},{"key":"app","value":"openshift-kube-apiserver"},{"key":"revision","value":"57"}],"creationTime":1685459718,"id":"7cb00651-2eb5-444b-b0b9-f1bf787e6724","type":"KubernetesPod","name":"kube-apiserver-control-2.demoenv.local","dome9Id":"11|5af13ab5-72ed-43b6-9544-33c9236d62f0|Pod|7cb00651-2eb5-444b-b0b9-f1bf787e6724","accountNumber":"5af13ab5-72ed-43b6-9544-33c9236d62f0","assetLabels":null,"region":"Global","externalFindings":null},"remediationActions":[],"action":"Detect","additionalFields":[]}