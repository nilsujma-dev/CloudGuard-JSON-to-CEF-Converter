{"status":"Failed","policy":{"name":"Webhooks","description":""},"findingKey":"5X2WD4aAjFgteJ/vFg0+nw","findingId":"5X2WD4aAjFgteJ/vFg0+nw","cloudGuardAccountId":"51722","origin":"Compliance Engine","bundle":{"name":"CIS Kubernetes Benchmark v1.6.1","description":"Automated Validation of Kubernetes CIS Benchmark v1.6.1 Prescriptive guidance for establishing a secure configuration posture for Kubernetes 1.6.1\nFor additional reference: https://www.cisecurity.org/benchmark/kubernetes/","id":-70},"reportTime":"2023-06-09T11:00:31.243Z","rule":{"name":"Ensure that the seccomp profile is set to docker/default in your pod definitions","ruleId":"D9.K8S.IAM.21","description":"Seccomp (secure computing mode) is used to restrict the set of system calls applications can make, allowing cluster administrators greater control over the security of workloads running in the cluster. Kubernetes disables seccomp profiles by default for historical reasons. You should enable it to ensure that the workloads have restricted actions available within the container.","remediation":"Use security context to enable the docker/default seccomp profile in your pod definitions.\n\n**References**\n1. https://github.com/kubernetes/kubernetes/issues/39845\n2. https://github.com/kubernetes/kubernetes/pull/21790\n3. https://github.com/kubernetes/community/blob/master/contributors/design-proposals/seccomp.md#examples\n4. https://docs.docker.com/engine/security/seccomp/","complianceTags":"5.6.2","logicHash":"wat89y5RGQIVJriupbowlQ","severity":"High"},"account":{"id":"5af13ab5-72ed-43b6-9544-33c9236d62f0","name":"nu-openshift-cluster","vendor":"Kubernetes","dome9CloudAccountId":"5af13ab5-72ed-43b6-9544-33c9236d62f0","organizationalUnitId":"00000000-0000-0000-0000-000000000000","organizationalUnitPath":""},"region":"Global","entity":{"spec":{"affinity":{"nodeAffinity":{"preferredDuringSchedulingIgnoredDuringExecution":null,"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchExpressions":null,"matchFields":[{"key":"metadata.name","operator":"In","values":["control-2.demoenv.local"]}]}]}},"podAffinity":null,"podAntiAffinity":null},"containers":[{"args":null,"command":["/bin/bash","-c","if [[ -f /env/_master ]]; then\n  set -o allexport\n  source /env/_master\n  set +o allexport\nfi\n\nexec openshift-sdn-controller \\\n --platform-type BareMetal \\\n --v=${OPENSHIFT_SDN_LOG_LEVEL:-2}\n"],"env":[{"name":"KUBERNETES_SERVICE_PORT","value":"6443","valueFrom":null},{"name":"KUBERNETES_SERVICE_HOST","value":"api-int.openshift-csa.demoenv.local","valueFrom":null}],"image":"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:d4f3f0f4d806c905b54760745793d79ffc737198bf05cabaf0a0fd6a6188441d","livenessProbe":null,"name":"sdn-controller","imageDetails":{"registry":"quay.io/openshift-release-dev","repository":"ocp-v4.0-art-dev","tag":"sha256:d4f3f0f4d806c905b54760745793d79ffc737198bf05cabaf0a0fd6a6188441d"},"parsedArgs":[{"key":"c","value":"if [["},{"key":"f","value":"/env/_master ]]; then  set"},{"key":"o","value":"allexport  source /env/_master  set +o allexportfiexec openshift-sdn-controller \\"},{"key":"platform-type","value":"BareMetal \\"},{"key":"v","value":"${OPENSHIFT_SDN_LOG_LEVEL:-2}"}],"ports":null,"readinessProbe":null,"resources":{"limits":null,"requests":{"memory":"50Mi","cpu":"10m"}},"securityContext":null,"volumeMounts":[{"name":"env-overrides","mountPath":"/env","readOnly":false,"mountPropagation":null,"subPath":null,"subPathExpr":null},{"name":"kube-api-access-74k4k","mountPath":"/var/run/secrets/kubernetes.io/serviceaccount","readOnly":true,"mountPropagation":null,"subPath":null,"subPathExpr":null}]},{"args":null,"command":["/bin/bash","-c","#!/bin/bash\nset -euo pipefail\nTLS_PK=/etc/pki/tls/metrics-certs/tls.key\nTLS_CERT=/etc/pki/tls/metrics-certs/tls.crt\n\n# As the secret mount is optional we must wait for the files to be present.\n# The service is created in monitor.yaml and this is created in controller.yaml.\n# If it isn't created there is probably an issue so we want to crashloop.\nTS=$(date +%s)\nWARN_TS=$(( ${TS} + $(( 20 * 60)) ))\nHAS_LOGGED_INFO=0\n\nlog_missing_certs(){\n    CUR_TS=$(date +%s)\n    if [[ \"${CUR_TS}\" -gt \"${WARN_TS}\"  ]]; then\n      echo $(date -Iseconds) WARN: sdn-controller-metrics-certs not mounted after 20 minutes.\n    elif [[ \"${HAS_LOGGED_INFO}\" -eq 0 ]] ; then\n      echo $(date -Iseconds) INFO: sdn-controller-metrics-certs not mounted. Waiting 20 minutes.\n      HAS_LOGGED_INFO=1\n    fi\n}\n\nwhile [[ ! -f \"${TLS_PK}\" ||  ! -f \"${TLS_CERT}\" ]] ; do\n  log_missing_certs\n  sleep 5\ndone\n\necho $(date -Iseconds) INFO: sdn-controller-metrics-certs mounted, starting kube-rbac-proxy\nexec /usr/bin/kube-rbac-proxy \\\n  --logtostderr \\\n  --secure-listen-address=:9106 \\\n  --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256 \\\n  --upstream=http://127.0.0.1:29100/ \\\n  --tls-private-key-file=${TLS_PK} \\\n  --tls-cert-file=${TLS_CERT}\n"],"env":null,"image":"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:9d9c95c485abf0c842ec6f7b0c82e05176b3f8351fe840ae5ffa4d91f0100fb7","livenessProbe":null,"name":"kube-rbac-proxy","imageDetails":{"registry":"quay.io/openshift-release-dev","repository":"ocp-v4.0-art-dev","tag":"sha256:9d9c95c485abf0c842ec6f7b0c82e05176b3f8351fe840ae5ffa4d91f0100fb7"},"parsedArgs":[{"key":"c","value":"#!/bin/bashset"},{"key":"euo","value":"pipefailTLS_PK=/etc/pki/tls/metrics-certs/tls.keyTLS_CERT=/etc/pki/tls/metrics-certs/tls.crt# As the secret mount is optional we must wait for the files to be present.# The service is created in monitor.yaml and this is created in controller.yaml.# If it isn't created there is probably an issue so we want to crashloop.TS=$(date +%s)WARN_TS=$(( ${TS} + $(( 20 * 60)) ))HAS_LOGGED_INFO=0log_missing_certs(){    CUR_TS=$(date +%s)    if [[ \"${CUR_TS}\""},{"key":"gt","value":"\"${WARN_TS}\"  ]]; then      echo $(date"},{"key":"Iseconds)","value":"WARN: sdn-controller-metrics-certs not mounted after 20 minutes.    elif [[ \"${HAS_LOGGED_INFO}\""},{"key":"eq","value":"0 ]] ; then      echo $(date"},{"key":"Iseconds)","value":"INFO: sdn-controller-metrics-certs not mounted. Waiting 20 minutes.      HAS_LOGGED_INFO=1    fi}while [[ !"},{"key":"f","value":"\"${TLS_PK}\" ||  !"},{"key":"f","value":"\"${TLS_CERT}\" ]] ; do  log_missing_certs  sleep 5doneecho $(date"},{"key":"Iseconds)","value":"INFO: sdn-controller-metrics-certs mounted, starting kube-rbac-proxyexec /usr/bin/kube-rbac-proxy \\ "},{"key":"logtostderr","value":"\\ "},{"key":"secure-listen-address","value":":9106 \\ "},{"key":"tls-cipher-suites","value":"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256 \\ "},{"key":"upstream","value":"http://127.0.0.1:29100/ \\ "},{"key":"tls-private-key-file","value":"${TLS_PK} \\ "},{"key":"tls-cert-file","value":"${TLS_CERT}"}],"ports":[{"containerPort":9106,"hostIP":null,"hostPort":9106,"name":"https","protocol":"TCP"}],"readinessProbe":null,"resources":{"limits":null,"requests":{"memory":"20Mi","cpu":"10m"}},"securityContext":null,"volumeMounts":[{"name":"sdn-controller-metrics-certs","mountPath":"/etc/pki/tls/metrics-certs","readOnly":true,"mountPropagation":null,"subPath":null,"subPathExpr":null},{"name":"kube-api-access-74k4k","mountPath":"/var/run/secrets/kubernetes.io/serviceaccount","readOnly":true,"mountPropagation":null,"subPath":null,"subPathExpr":null}]}],"dnsConfig":null,"hostAliases":null,"hostIPC":false,"hostNetwork":true,"hostPID":false,"initContainers":null,"nodeInfo":{"conditions":[{"message":"kubelet has sufficient memory available","reason":"KubeletHasSufficientMemory","status":"False","type":"MemoryPressure"},{"message":"kubelet has no disk pressure","reason":"KubeletHasNoDiskPressure","status":"False","type":"DiskPressure"},{"message":"kubelet has sufficient PID available","reason":"KubeletHasSufficientPID","status":"False","type":"PIDPressure"},{"message":"kubelet is posting ready status","reason":"KubeletReady","status":"True","type":"Ready"}],"labels":[{"key":"beta.kubernetes.io/arch","value":"amd64"},{"key":"beta.kubernetes.io/os","value":"linux"},{"key":"kubernetes.io/arch","value":"amd64"},{"key":"kubernetes.io/hostname","value":"control-2.demoenv.local"},{"key":"kubernetes.io/os","value":"linux"},{"key":"node-role.kubernetes.io/master","value":""},{"key":"node.openshift.io/os_id","value":"rhcos"}]},"nodeName":"control-2.demoenv.local","priority":2000000000,"priorityClassName":"system-cluster-critical","securityContext":{"fsGroup":null,"runAsGroup":null,"runAsNonRoot":true,"runAsUser":65534,"seLinuxOptions":null,"supplementalGroups":null,"sysctls":null,"windowsOptions":null,"seccompProfile":null},"serviceAccount":"sdn-controller","serviceAccountName":"sdn-controller","tolerations":[{"key":"node-role.kubernetes.io/master","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/network-unavailable","operator":"Exists","effect":"NoSchedule"}],"volumes":[{"hostPath":null,"flexVolume":null,"name":"env-overrides","persistentVolumeClaim":null},{"hostPath":null,"flexVolume":null,"name":"sdn-controller-metrics-certs","persistentVolumeClaim":null},{"hostPath":null,"flexVolume":null,"name":"kube-api-access-74k4k","persistentVolumeClaim":null}]},"status":{"phase":"Running","podIP":"192.168.10.12"},"networkPolicies":{"ingress":[],"egress":[]},"owner":{"ownerReferences":[{"kind":"DaemonSet","uid":"854ee97b-f906-40cf-af01-bd8bef402902","name":"sdn-controller"}],"rootOwner":{"kind":"DaemonSet","uid":"854ee97b-f906-40cf-af01-bd8bef402902","name":"sdn-controller"}},"tags":[{"key":"app","value":"sdn-controller"},{"key":"controller-revision-hash","value":"76c7cb9897"},{"key":"pod-template-generation","value":"2"}],"namespace":"openshift-sdn","annotations":[],"labels":[{"key":"app","value":"sdn-controller"},{"key":"controller-revision-hash","value":"76c7cb9897"},{"key":"pod-template-generation","value":"2"}],"creationTime":1671596966,"id":"6ba7b85f-d1c3-4b59-82e6-fd6d3e913116","type":"KubernetesPod","name":"sdn-controller-jn8wn","dome9Id":"11|5af13ab5-72ed-43b6-9544-33c9236d62f0|Pod|6ba7b85f-d1c3-4b59-82e6-fd6d3e913116","accountNumber":"5af13ab5-72ed-43b6-9544-33c9236d62f0","assetLabels":null,"region":"Global","externalFindings":null},"remediationActions":[],"action":"Detect","additionalFields":[]}