{"status":"Failed","policy":{"name":"Webhooks","description":""},"findingKey":"7G4DgPvTlAG6QqpFlFyyiw","findingId":"7G4DgPvTlAG6QqpFlFyyiw","cloudGuardAccountId":"51722","origin":"Compliance Engine","bundle":{"name":"CIS Kubernetes Benchmark v1.6.1","description":"Automated Validation of Kubernetes CIS Benchmark v1.6.1 Prescriptive guidance for establishing a secure configuration posture for Kubernetes 1.6.1\nFor additional reference: https://www.cisecurity.org/benchmark/kubernetes/","id":-70},"reportTime":"2023-06-09T11:00:31.243Z","rule":{"name":"Apply Security Context to Your Pods and Containers","ruleId":"D9.K8S.OPE.06","description":"Apply Security Context to Your Pods and Containers. A security context defines the operating system security settings (uid, gid, capabilities, SELinux role, etc..) applied to a container. When designing your containers and pods, make sure that you configure the security context for your pods, containers, and volumes. A security context is a property defined in the deployment yaml. It controls the security parameters that will be assigned to the pod/container/volume.","remediation":"Follow the Kubernetes documentation and apply security contexts to your pods. For a suggested list of security contexts, you may refer to the CIS Security Benchmark for Docker Containers.\n**References**\nhttps://kubernetes.io/docs/concepts/policy/security-context/","complianceTags":"5.6.3","logicHash":"IESpWUCb45AmMIPF+zZBxA","severity":"Low"},"account":{"id":"5af13ab5-72ed-43b6-9544-33c9236d62f0","name":"nu-openshift-cluster","vendor":"Kubernetes","dome9CloudAccountId":"5af13ab5-72ed-43b6-9544-33c9236d62f0","organizationalUnitId":"00000000-0000-0000-0000-000000000000","organizationalUnitPath":""},"region":"Global","entity":{"spec":{"affinity":null,"containers":[{"args":["LOCK=/var/log/kube-apiserver/.lock\n# We should be able to acquire the lock immediatelly. If not, it means the init container has not released it yet and kubelet or CRI-O started container prematurely.\nexec {LOCK_FD}>${LOCK} && flock --verbose -w 30 \"${LOCK_FD}\" || {\n  echo \"Failed to acquire lock for kube-apiserver. Please check setup container for details. This is likely kubelet or CRI-O bug.\"\n  exit 1\n}\nif [ -f /etc/kubernetes/static-pod-certs/configmaps/trusted-ca-bundle/ca-bundle.crt ]; then\n  echo \"Copying system trust bundle ...\"\n  cp -f /etc/kubernetes/static-pod-certs/configmaps/trusted-ca-bundle/ca-bundle.crt /etc/pki/ca-trust/extracted/pem/tls-ca-bundle.pem\nfi\n\nexec watch-termination --termination-touch-file=/var/log/kube-apiserver/.terminating --termination-log-file=/var/log/kube-apiserver/termination.log --graceful-termination-duration=135s --kubeconfig=/etc/kubernetes/static-pod-resources/configmaps/kube-apiserver-cert-syncer-kubeconfig/kubeconfig -- hyperkube kube-apiserver --openshift-config=/etc/kubernetes/static-pod-resources/configmaps/config/config.yaml --advertise-address=${HOST_IP}  -v=2 --permit-address-sharing\n"],"command":["/bin/bash","-ec"],"env":[{"name":"POD_NAME","value":null,"valueFrom":{"secretKeyRef":null}},{"name":"POD_NAMESPACE","value":null,"valueFrom":{"secretKeyRef":null}},{"name":"STATIC_POD_VERSION","value":"57","valueFrom":null},{"name":"HOST_IP","value":null,"valueFrom":{"secretKeyRef":null}},{"name":"GOGC","value":"100","valueFrom":null}],"image":"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:7dd9070ddd5c6d877e6431af587973c39f97f753f2c504b43c4841d5ac3f3733","livenessProbe":{"exec":null,"failureThreshold":3,"httpGet":{"host":null,"path":"livez","port":"6443","scheme":"HTTPS","httpHeaders":null},"initialDelaySeconds":45,"periodSeconds":10,"successThreshold":1,"tcpSocket":null,"timeoutSeconds":10},"name":"kube-apiserver","imageDetails":{"registry":"quay.io/openshift-release-dev","repository":"ocp-v4.0-art-dev","tag":"sha256:7dd9070ddd5c6d877e6431af587973c39f97f753f2c504b43c4841d5ac3f3733"},"parsedArgs":[{"key":"ec","value":"LOCK=/var/log/kube-apiserver/.lock# We should be able to acquire the lock immediatelly. If not, it means the init container has not released it yet and kubelet or CRI-O started container prematurely.exec {LOCK_FD}>${LOCK} && flock"},{"key":"w","value":"30 \"${LOCK_FD}\" || {  echo \"Failed to acquire lock for kube-apiserver. Please check setup container for details. This is likely kubelet or CRI-O bug.\"  exit 1}if ["},{"key":"f","value":"/etc/kubernetes/static-pod-certs/configmaps/trusted-ca-bundle/ca-bundle.crt ]; then  echo \"Copying system trust bundle ...\"  cp"},{"key":"f","value":"/etc/kubernetes/static-pod-certs/configmaps/trusted-ca-bundle/ca-bundle.crt /etc/pki/ca-trust/extracted/pem/tls-ca-bundle.pemfiexec watch-termination"},{"key":"termination-touch-file","value":"/var/log/kube-apiserver/.terminating"},{"key":"termination-log-file","value":"/var/log/kube-apiserver/termination.log"},{"key":"graceful-termination-duration","value":"135s"},{"key":"kubeconfig","value":"/etc/kubernetes/static-pod-resources/configmaps/kube-apiserver-cert-syncer-kubeconfig/kubeconfig"},{"key":"","value":"hyperkube kube-apiserver"},{"key":"openshift-config","value":"/etc/kubernetes/static-pod-resources/configmaps/config/config.yaml"},{"key":"advertise-address","value":"${HOST_IP} "},{"key":"v","value":"2"}],"ports":[{"containerPort":6443,"hostIP":null,"hostPort":6443,"name":null,"protocol":"TCP"}],"readinessProbe":{"exec":null,"failureThreshold":3,"httpGet":{"host":null,"path":"readyz","port":"6443","scheme":"HTTPS","httpHeaders":null},"initialDelaySeconds":10,"periodSeconds":10,"successThreshold":1,"tcpSocket":null,"timeoutSeconds":10},"resources":{"limits":null,"requests":{"memory":"1Gi","cpu":"265m"}},"securityContext":{"allowPrivilegeEscalation":null,"capabilities":null,"privileged":true,"procMount":null,"readOnlyRootFilesystem":null,"runAsGroup":null,"runAsNonRoot":null,"runAsUser":null,"seLinuxOptions":null,"windowsOptions":null,"seccompProfile":null},"volumeMounts":[{"name":"resource-dir","mountPath":"/etc/kubernetes/static-pod-resources","readOnly":false,"mountPropagation":null,"subPath":null,"subPathExpr":null},{"name":"cert-dir","mountPath":"/etc/kubernetes/static-pod-certs","readOnly":false,"mountPropagation":null,"subPath":null,"subPathExpr":null},{"name":"audit-dir","mountPath":"/var/log/kube-apiserver","readOnly":false,"mountPropagation":null,"subPath":null,"subPathExpr":null}]},{"args":["--kubeconfig=/etc/kubernetes/static-pod-resources/configmaps/kube-apiserver-cert-syncer-kubeconfig/kubeconfig","--namespace=$(POD_NAMESPACE)","--destination-dir=/etc/kubernetes/static-pod-certs"],"command":["cluster-kube-apiserver-operator","cert-syncer"],"env":[{"name":"POD_NAME","value":null,"valueFrom":{"secretKeyRef":null}},{"name":"POD_NAMESPACE","value":null,"valueFrom":{"secretKeyRef":null}}],"image":"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3ba4b4f00df0329a9a84a848b4289f2cb2ee3aed069de92f7fae4f0928b35082","livenessProbe":null,"name":"kube-apiserver-cert-syncer","imageDetails":{"registry":"quay.io/openshift-release-dev","repository":"ocp-v4.0-art-dev","tag":"sha256:3ba4b4f00df0329a9a84a848b4289f2cb2ee3aed069de92f7fae4f0928b35082"},"parsedArgs":[{"key":"kube-apiserver-operator","value":"cert-syncer"},{"key":"kubeconfig","value":"/etc/kubernetes/static-pod-resources/configmaps/kube-apiserver-cert-syncer-kubeconfig/kubeconfig"},{"key":"namespace","value":"$(POD_NAMESPACE)"},{"key":"destination-dir","value":"/etc/kubernetes/static-pod-certs"}],"ports":null,"readinessProbe":null,"resources":{"limits":null,"requests":{"memory":"50Mi","cpu":"5m"}},"securityContext":null,"volumeMounts":[{"name":"resource-dir","mountPath":"/etc/kubernetes/static-pod-resources","readOnly":false,"mountPropagation":null,"subPath":null,"subPathExpr":null},{"name":"cert-dir","mountPath":"/etc/kubernetes/static-pod-certs","readOnly":false,"mountPropagation":null,"subPath":null,"subPathExpr":null}]},{"args":["--kubeconfig=/etc/kubernetes/static-pod-resources/configmaps/kube-apiserver-cert-syncer-kubeconfig/kubeconfig","--namespace=$(POD_NAMESPACE)","-v=2"],"command":["cluster-kube-apiserver-operator","cert-regeneration-controller"],"env":[{"name":"POD_NAMESPACE","value":null,"valueFrom":{"secretKeyRef":null}}],"image":"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3ba4b4f00df0329a9a84a848b4289f2cb2ee3aed069de92f7fae4f0928b35082","livenessProbe":null,"name":"kube-apiserver-cert-regeneration-controller","imageDetails":{"registry":"quay.io/openshift-release-dev","repository":"ocp-v4.0-art-dev","tag":"sha256:3ba4b4f00df0329a9a84a848b4289f2cb2ee3aed069de92f7fae4f0928b35082"},"parsedArgs":[{"key":"kube-apiserver-operator","value":"cert-regeneration-controller"},{"key":"kubeconfig","value":"/etc/kubernetes/static-pod-resources/configmaps/kube-apiserver-cert-syncer-kubeconfig/kubeconfig"},{"key":"namespace","value":"$(POD_NAMESPACE)"},{"key":"v","value":"2"}],"ports":null,"readinessProbe":null,"resources":{"limits":null,"requests":{"memory":"50Mi","cpu":"5m"}},"securityContext":null,"volumeMounts":[{"name":"resource-dir","mountPath":"/etc/kubernetes/static-pod-resources","readOnly":false,"mountPropagation":null,"subPath":null,"subPathExpr":null}]},{"args":["--insecure-port=6080","--delegate-url=https://localhost:6443/readyz"],"command":["cluster-kube-apiserver-operator","insecure-readyz"],"env":null,"image":"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3ba4b4f00df0329a9a84a848b4289f2cb2ee3aed069de92f7fae4f0928b35082","livenessProbe":null,"name":"kube-apiserver-insecure-readyz","imageDetails":{"registry":"quay.io/openshift-release-dev","repository":"ocp-v4.0-art-dev","tag":"sha256:3ba4b4f00df0329a9a84a848b4289f2cb2ee3aed069de92f7fae4f0928b35082"},"parsedArgs":[{"key":"kube-apiserver-operator","value":"insecure-readyz"},{"key":"insecure-port","value":"6080"},{"key":"delegate-url","value":"https://localhost:6443/readyz"}],"ports":[{"containerPort":6080,"hostIP":null,"hostPort":6080,"name":null,"protocol":"TCP"}],"readinessProbe":null,"resources":{"limits":null,"requests":{"memory":"50Mi","cpu":"5m"}},"securityContext":null,"volumeMounts":null},{"args":["--kubeconfig","/etc/kubernetes/static-pod-certs/configmaps/check-endpoints-kubeconfig/kubeconfig","--listen","0.0.0.0:17697","--namespace","$(POD_NAMESPACE)","--v","2"],"command":["cluster-kube-apiserver-operator","check-endpoints"],"env":[{"name":"POD_NAME","value":null,"valueFrom":{"secretKeyRef":null}},{"name":"POD_NAMESPACE","value":null,"valueFrom":{"secretKeyRef":null}}],"image":"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:3ba4b4f00df0329a9a84a848b4289f2cb2ee3aed069de92f7fae4f0928b35082","livenessProbe":{"exec":null,"failureThreshold":3,"httpGet":{"host":null,"path":"healthz","port":"17697","scheme":"HTTPS","httpHeaders":null},"initialDelaySeconds":10,"periodSeconds":10,"successThreshold":1,"tcpSocket":null,"timeoutSeconds":10},"name":"kube-apiserver-check-endpoints","imageDetails":{"registry":"quay.io/openshift-release-dev","repository":"ocp-v4.0-art-dev","tag":"sha256:3ba4b4f00df0329a9a84a848b4289f2cb2ee3aed069de92f7fae4f0928b35082"},"parsedArgs":[{"key":"kube-apiserver-operator","value":"check-endpoints"},{"key":"kubeconfig","value":"/etc/kubernetes/static-pod-certs/configmaps/check-endpoints-kubeconfig/kubeconfig"},{"key":"listen","value":"0.0.0.0:17697"},{"key":"namespace","value":"$(POD_NAMESPACE)"},{"key":"v","value":"2"}],"ports":[{"containerPort":17697,"hostIP":null,"hostPort":17697,"name":"check-endpoints","protocol":"TCP"}],"readinessProbe":{"exec":null,"failureThreshold":3,"httpGet":{"host":null,"path":"healthz","port":"17697","scheme":"HTTPS","httpHeaders":null},"initialDelaySeconds":10,"periodSeconds":10,"successThreshold":1,"tcpSocket":null,"timeoutSeconds":10},"resources":{"limits":null,"requests":{"memory":"50Mi","cpu":"10m"}},"securityContext":null,"volumeMounts":[{"name":"resource-dir","mountPath":"/etc/kubernetes/static-pod-resources","readOnly":false,"mountPropagation":null,"subPath":null,"subPathExpr":null},{"name":"cert-dir","mountPath":"/etc/kubernetes/static-pod-certs","readOnly":false,"mountPropagation":null,"subPath":null,"subPathExpr":null}]}],"dnsConfig":null,"hostAliases":null,"hostIPC":false,"hostNetwork":true,"hostPID":false,"initContainers":[{"args":["echo \"Fixing audit permissions ...\"\nchmod 0700 /var/log/kube-apiserver && touch /var/log/kube-apiserver/audit.log && chmod 0600 /var/log/kube-apiserver/*\n\nLOCK=/var/log/kube-apiserver/.lock\necho \"Acquiring exclusive lock ${LOCK} ...\"\n\n# Waiting for 135s max for old kube-apiserver's watch-termination process to exit and remove the lock.\n# Two cases:\n# 1. if kubelet does not start the old and new in parallel (i.e. works as expected), the flock will always succeed without any time.\n# 2. if kubelet does overlap old and new pods for up to 130s, the flock will wait and immediate return when the old finishes.\n#\n# NOTE: We can increase 135s for a bigger expected overlap. But a higher value means less noise about the broken kubelet behaviour, i.e. we hide a bug.\n# NOTE: Do not tweak these timings without considering the livenessProbe initialDelaySeconds\nexec {LOCK_FD}>${LOCK} && flock --verbose -w 135 \"${LOCK_FD}\" || {\n  echo \"$(date -Iseconds -u) kubelet did not terminate old kube-apiserver before new one\" >> /var/log/kube-apiserver/lock.log\n  echo -n \": WARNING: kubelet did not terminate old kube-apiserver before new one.\"\n\n  # We failed to acquire exclusive lock, which means there is old kube-apiserver running in system.\n  # Since we utilize SO_REUSEPORT, we need to make sure the old kube-apiserver stopped listening.\n  #\n  # NOTE: This is a fallback for broken kubelet, if you observe this please report a bug.\n  echo -n \"Waiting for port 6443 to be released due to likely bug in kubelet or CRI-O \"\n  while [ -n \"$(ss -Htan state listening '( sport = 6443 or sport = 6080 )')\" ]; do\n    echo -n \".\"\n    sleep 1\n    (( tries += 1 ))\n    if [[ \"${tries}\" -gt 10 ]]; then\n      echo \"Timed out waiting for port :6443 and :6080 to be released, this is likely a bug in kubelet or CRI-O\"\n      exit 1\n    fi\n  done\n  #  This is to make sure the server has terminated independently from the lock.\n  #  After the port has been freed (requests can be pending and need 60s max).\n  sleep 65\n}\n# We cannot hold the lock from the init container to the main container. We release it here. There is no risk, at this point we know we are safe.\nflock -u \"${LOCK_FD}\"\n"],"command":["/usr/bin/timeout","220","/bin/bash","-ec"],"env":null,"image":"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:7dd9070ddd5c6d877e6431af587973c39f97f753f2c504b43c4841d5ac3f3733","livenessProbe":null,"name":"setup","imageDetails":{"registry":"quay.io/openshift-release-dev","repository":"ocp-v4.0-art-dev","tag":"sha256:7dd9070ddd5c6d877e6431af587973c39f97f753f2c504b43c4841d5ac3f3733"},"parsedArgs":[{"key":"ec","value":"echo \"Fixing audit permissions ...\"chmod 0700 /var/log/kube-apiserver && touch /var/log/kube-apiserver/audit.log && chmod 0600 /var/log/kube-apiserver/*LOCK=/var/log/kube-apiserver/.lockecho \"Acquiring exclusive lock ${LOCK} ...\"# Waiting for 135s max for old kube-apiserver's watch-termination process to exit and remove the lock.# Two cases:# 1. if kubelet does not start the old and new in parallel (i.e. works as expected), the flock will always succeed without any time.# 2. if kubelet does overlap old and new pods for up to 130s, the flock will wait and immediate return when the old finishes.## NOTE: We can increase 135s for a bigger expected overlap. But a higher value means less noise about the broken kubelet behaviour, i.e. we hide a bug.# NOTE: Do not tweak these timings without considering the livenessProbe initialDelaySecondsexec {LOCK_FD}>${LOCK} && flock"},{"key":"w","value":"135 \"${LOCK_FD}\" || {  echo \"$(date"},{"key":"u)","value":"kubelet did not terminate old kube-apiserver before new one\" >> /var/log/kube-apiserver/lock.log  echo"},{"key":"n","value":"\": WARNING: kubelet did not terminate old kube-apiserver before new one.\"  # We failed to acquire exclusive lock, which means there is old kube-apiserver running in system.  # Since we utilize SO_REUSEPORT, we need to make sure the old kube-apiserver stopped listening.  #  # NOTE: This is a fallback for broken kubelet, if you observe this please report a bug.  echo"},{"key":"n","value":"\"Waiting for port 6443 to be released due to likely bug in kubelet or CRI-O \"  while ["},{"key":"n","value":"\"$(ss"},{"key":"Htan","value":"state listening '( sport = 6443 or sport = 6080 )')\" ]; do    echo"},{"key":"n","value":"\".\"    sleep 1    (( tries += 1 ))    if [[ \"${tries}\""},{"key":"gt","value":"10 ]]; then      echo \"Timed out waiting for port :6443 and :6080 to be released, this is likely a bug in kubelet or CRI-O\"      exit 1    fi  done  #  This is to make sure the server has terminated independently from the lock.  #  After the port has been freed (requests can be pending and need 60s max).  sleep 65}# We cannot hold the lock from the init container to the main container. We release it here. There is no risk, at this point we know we are safe.flock"},{"key":"u","value":"\"${LOCK_FD}\""}],"ports":null,"readinessProbe":null,"resources":{"limits":null,"requests":{"memory":"50Mi","cpu":"5m"}},"securityContext":{"allowPrivilegeEscalation":null,"capabilities":null,"privileged":true,"procMount":null,"readOnlyRootFilesystem":null,"runAsGroup":null,"runAsNonRoot":null,"runAsUser":null,"seLinuxOptions":null,"windowsOptions":null,"seccompProfile":null},"volumeMounts":[{"name":"audit-dir","mountPath":"/var/log/kube-apiserver","readOnly":false,"mountPropagation":null,"subPath":null,"subPathExpr":null}]}],"nodeInfo":{"conditions":[{"message":"kubelet has sufficient memory available","reason":"KubeletHasSufficientMemory","status":"False","type":"MemoryPressure"},{"message":"kubelet has no disk pressure","reason":"KubeletHasNoDiskPressure","status":"False","type":"DiskPressure"},{"message":"kubelet has sufficient PID available","reason":"KubeletHasSufficientPID","status":"False","type":"PIDPressure"},{"message":"kubelet is posting ready status","reason":"KubeletReady","status":"True","type":"Ready"}],"labels":[{"key":"beta.kubernetes.io/arch","value":"amd64"},{"key":"beta.kubernetes.io/os","value":"linux"},{"key":"kubernetes.io/arch","value":"amd64"},{"key":"kubernetes.io/hostname","value":"control-1.demoenv.local"},{"key":"kubernetes.io/os","value":"linux"},{"key":"node-role.kubernetes.io/master","value":""},{"key":"node.openshift.io/os_id","value":"rhcos"}]},"nodeName":"control-1.demoenv.local","priority":2000001000,"priorityClassName":"system-node-critical","securityContext":{"fsGroup":null,"runAsGroup":null,"runAsNonRoot":null,"runAsUser":null,"seLinuxOptions":null,"supplementalGroups":null,"sysctls":null,"windowsOptions":null,"seccompProfile":null},"serviceAccount":null,"serviceAccountName":null,"tolerations":[{"key":null,"operator":"Exists","effect":null}],"volumes":[{"hostPath":{"path":"/etc/kubernetes/static-pod-resources/kube-apiserver-pod-57","type":""},"flexVolume":null,"name":"resource-dir","persistentVolumeClaim":null},{"hostPath":{"path":"/etc/kubernetes/static-pod-resources/kube-apiserver-certs","type":""},"flexVolume":null,"name":"cert-dir","persistentVolumeClaim":null},{"hostPath":{"path":"/var/log/kube-apiserver","type":""},"flexVolume":null,"name":"audit-dir","persistentVolumeClaim":null}]},"status":{"phase":"Running","podIP":"192.168.10.11"},"networkPolicies":{"ingress":[],"egress":[]},"owner":{"ownerReferences":[{"kind":"Node","uid":"abddf8bc-d0d0-42d8-9b1f-8dbe92ca0cbe","name":"control-1.demoenv.local"}],"rootOwner":{"kind":"Node","uid":"abddf8bc-d0d0-42d8-9b1f-8dbe92ca0cbe","name":"control-1.demoenv.local"}},"tags":[{"key":"apiserver","value":"true"},{"key":"app","value":"openshift-kube-apiserver"},{"key":"revision","value":"57"}],"namespace":"openshift-kube-apiserver","annotations":[{"key":"kubectl.kubernetes.io/default-container","value":"kube-apiserver"},{"key":"kubernetes.io/config.hash","value":"090c5c50685e9c861a76ba7b19ee321b"},{"key":"kubernetes.io/config.mirror","value":"090c5c50685e9c861a76ba7b19ee321b"},{"key":"kubernetes.io/config.seen","value":"2023-05-30T15:21:40.547627476Z"},{"key":"kubernetes.io/config.source","value":"file"},{"key":"target.workload.openshift.io/management","value":"{\"effect\": \"PreferredDuringScheduling\"}"}],"labels":[{"key":"apiserver","value":"true"},{"key":"app","value":"openshift-kube-apiserver"},{"key":"revision","value":"57"}],"creationTime":1685460175,"id":"504a8fbb-5feb-485a-a3f0-f80dd015d524","type":"KubernetesPod","name":"kube-apiserver-control-1.demoenv.local","dome9Id":"11|5af13ab5-72ed-43b6-9544-33c9236d62f0|Pod|504a8fbb-5feb-485a-a3f0-f80dd015d524","accountNumber":"5af13ab5-72ed-43b6-9544-33c9236d62f0","assetLabels":null,"region":"Global","externalFindings":null},"remediationActions":[],"action":"Detect","additionalFields":[]}