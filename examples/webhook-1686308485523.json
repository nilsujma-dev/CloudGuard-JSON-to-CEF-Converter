{"status":"Failed","policy":{"name":"Webhooks","description":""},"findingKey":"c9V5F1MhIB0TelPN0ds5zg","findingId":"c9V5F1MhIB0TelPN0ds5zg","cloudGuardAccountId":"51722","origin":"Compliance Engine","bundle":{"name":"CIS Kubernetes Benchmark v1.6.1","description":"Automated Validation of Kubernetes CIS Benchmark v1.6.1 Prescriptive guidance for establishing a secure configuration posture for Kubernetes 1.6.1\nFor additional reference: https://www.cisecurity.org/benchmark/kubernetes/","id":-70},"reportTime":"2023-06-09T11:00:31.243Z","rule":{"name":"Ensure that the seccomp profile is set to docker/default in your pod definitions","ruleId":"D9.K8S.IAM.21","description":"Seccomp (secure computing mode) is used to restrict the set of system calls applications can make, allowing cluster administrators greater control over the security of workloads running in the cluster. Kubernetes disables seccomp profiles by default for historical reasons. You should enable it to ensure that the workloads have restricted actions available within the container.","remediation":"Use security context to enable the docker/default seccomp profile in your pod definitions.\n\n**References**\n1. https://github.com/kubernetes/kubernetes/issues/39845\n2. https://github.com/kubernetes/kubernetes/pull/21790\n3. https://github.com/kubernetes/community/blob/master/contributors/design-proposals/seccomp.md#examples\n4. https://docs.docker.com/engine/security/seccomp/","complianceTags":"5.6.2","logicHash":"wat89y5RGQIVJriupbowlQ","severity":"High"},"account":{"id":"5af13ab5-72ed-43b6-9544-33c9236d62f0","name":"nu-openshift-cluster","vendor":"Kubernetes","dome9CloudAccountId":"5af13ab5-72ed-43b6-9544-33c9236d62f0","organizationalUnitId":"00000000-0000-0000-0000-000000000000","organizationalUnitPath":""},"region":"Global","entity":{"spec":{"affinity":null,"containers":[{"args":null,"command":["/bin/bash","-c","#/bin/bash\nverify_old_haproxy_ps_being_deleted()\n{\n  local prev_pids\n\n  prev_pids=\"$1\"\n  sleep $OLD_HAPROXY_PS_FORCE_DEL_TIMEOUT\n  cur_pids=$(pidof haproxy)\n\n  for val in $prev_pids; do\n      if [[ $cur_pids =~ (^|[[:space:]])\"$val\"($|[[:space:]]) ]] ; then\n         kill $val\n      fi\n  done\n}\n\nreload_haproxy()\n{\n  old_pids=$(pidof haproxy)\n  if [ -n \"$old_pids\" ]; then\n      /usr/sbin/haproxy -W -db -f /etc/haproxy/haproxy.cfg  -p /var/lib/haproxy/run/haproxy.pid -x /var/lib/haproxy/run/haproxy.sock -sf $old_pids &\n      #There seems to be some cases where HAProxy doesn't drain properly.\n      #To handle that case, SIGTERM signal being sent to old HAProxy processes which haven't terminated.\n      verify_old_haproxy_ps_being_deleted \"$old_pids\"  &\n  else\n      /usr/sbin/haproxy -W -db -f /etc/haproxy/haproxy.cfg  -p /var/lib/haproxy/run/haproxy.pid &\n  fi\n}\n\nmsg_handler()\n{\n  while read -r line; do\n    echo \"The client send: $line\"  >&2\n    # currently only 'reload' msg is supported\n    if [ \"$line\" = reload ]; then\n        reload_haproxy\n    fi\n  done\n}\nset -ex\ndeclare -r haproxy_sock=\"/var/run/haproxy/haproxy-master.sock\"\ndeclare -r haproxy_log_sock=\"/var/run/haproxy/haproxy-log.sock\"\nexport -f msg_handler\nexport -f reload_haproxy\nexport -f verify_old_haproxy_ps_being_deleted\nrm -f \"$haproxy_sock\" \"$haproxy_log_sock\"\nsocat UNIX-RECV:${haproxy_log_sock} STDOUT &\nif [ -s \"/etc/haproxy/haproxy.cfg\" ]; then\n    /usr/sbin/haproxy -W -db -f /etc/haproxy/haproxy.cfg  -p /var/lib/haproxy/run/haproxy.pid &\nfi\nsocat UNIX-LISTEN:${haproxy_sock},fork system:'bash -c msg_handler'\n"],"env":[{"name":"OLD_HAPROXY_PS_FORCE_DEL_TIMEOUT","value":"120","valueFrom":null}],"image":"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:a96a7de25e48331facd162fd77bdb2cd083cebbe78dfbf4cab02fb778127024e","livenessProbe":{"exec":null,"failureThreshold":3,"httpGet":{"host":null,"path":"/haproxy_ready","port":"9444","scheme":"HTTP","httpHeaders":null},"initialDelaySeconds":50,"periodSeconds":10,"successThreshold":1,"tcpSocket":null,"timeoutSeconds":1},"name":"haproxy","imageDetails":{"registry":"quay.io/openshift-release-dev","repository":"ocp-v4.0-art-dev","tag":"sha256:a96a7de25e48331facd162fd77bdb2cd083cebbe78dfbf4cab02fb778127024e"},"parsedArgs":[{"key":"c","value":"#/bin/bashverify_old_haproxy_ps_being_deleted(){  local prev_pids  prev_pids=\"$1\"  sleep $OLD_HAPROXY_PS_FORCE_DEL_TIMEOUT  cur_pids=$(pidof haproxy)  for val in $prev_pids; do      if [[ $cur_pids =~ (^|[[:space:]])\"$val\"($|[[:space:]]) ]] ; then         kill $val      fi  done}reload_haproxy(){  old_pids=$(pidof haproxy)  if ["},{"key":"n","value":"\"$old_pids\" ]; then      /usr/sbin/haproxy"},{"key":"f","value":"/etc/haproxy/haproxy.cfg "},{"key":"p","value":"/var/lib/haproxy/run/haproxy.pid"},{"key":"x","value":"/var/lib/haproxy/run/haproxy.sock"},{"key":"sf","value":"$old_pids &      #There seems to be some cases where HAProxy doesn't drain properly.      #To handle that case, SIGTERM signal being sent to old HAProxy processes which haven't terminated.      verify_old_haproxy_ps_being_deleted \"$old_pids\"  &  else      /usr/sbin/haproxy"},{"key":"f","value":"/etc/haproxy/haproxy.cfg "},{"key":"p","value":"/var/lib/haproxy/run/haproxy.pid &  fi}msg_handler(){  while read"},{"key":"r","value":"line; do    echo \"The client send: $line\"  >&2    # currently only 'reload' msg is supported    if [ \"$line\" = reload ]; then        reload_haproxy    fi  done}set"},{"key":"r","value":"haproxy_sock=\"/var/run/haproxy/haproxy-master.sock\"declare"},{"key":"r","value":"haproxy_log_sock=\"/var/run/haproxy/haproxy-log.sock\"export"},{"key":"f","value":"msg_handlerexport"},{"key":"f","value":"reload_haproxyexport"},{"key":"f","value":"verify_old_haproxy_ps_being_deletedrm"},{"key":"f","value":"\"$haproxy_sock\" \"$haproxy_log_sock\"socat UNIX-RECV:${haproxy_log_sock} STDOUT &if ["},{"key":"s","value":"\"/etc/haproxy/haproxy.cfg\" ]; then    /usr/sbin/haproxy"},{"key":"f","value":"/etc/haproxy/haproxy.cfg "},{"key":"p","value":"/var/lib/haproxy/run/haproxy.pid &fisocat UNIX-LISTEN:${haproxy_sock},fork system:'bash"},{"key":"c","value":"msg_handler'"}],"ports":null,"readinessProbe":null,"resources":{"limits":null,"requests":{"memory":"200Mi","cpu":"100m"}},"securityContext":null,"volumeMounts":[{"name":"conf-dir","mountPath":"/etc/haproxy","readOnly":false,"mountPropagation":null,"subPath":null,"subPathExpr":null},{"name":"run-dir","mountPath":"/var/run/haproxy","readOnly":false,"mountPropagation":null,"subPath":null,"subPathExpr":null}]},{"args":null,"command":["monitor","/var/lib/kubelet/kubeconfig","/config/haproxy.cfg.tmpl","/etc/haproxy/haproxy.cfg","--api-vip","192.168.10.17"],"env":null,"image":"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1e747154a68ae161551581454ef86237b20192bd0ebf72a7ea4e4b8a9ab0f242","livenessProbe":null,"name":"haproxy-monitor","imageDetails":{"registry":"quay.io/openshift-release-dev","repository":"ocp-v4.0-art-dev","tag":"sha256:1e747154a68ae161551581454ef86237b20192bd0ebf72a7ea4e4b8a9ab0f242"},"parsedArgs":[{"key":"api-vip","value":"192.168.10.17"}],"ports":null,"readinessProbe":null,"resources":{"limits":null,"requests":{"memory":"200Mi","cpu":"100m"}},"securityContext":{"allowPrivilegeEscalation":null,"capabilities":null,"privileged":true,"procMount":null,"readOnlyRootFilesystem":null,"runAsGroup":null,"runAsNonRoot":null,"runAsUser":null,"seLinuxOptions":null,"windowsOptions":null,"seccompProfile":null},"volumeMounts":[{"name":"conf-dir","mountPath":"/etc/haproxy","readOnly":false,"mountPropagation":null,"subPath":null,"subPathExpr":null},{"name":"run-dir","mountPath":"/var/run/haproxy","readOnly":false,"mountPropagation":null,"subPath":null,"subPathExpr":null},{"name":"resource-dir","mountPath":"/config","readOnly":false,"mountPropagation":null,"subPath":null,"subPathExpr":null},{"name":"chroot-host","mountPath":"/host","readOnly":false,"mountPropagation":null,"subPath":null,"subPathExpr":null},{"name":"kubeconfigvarlib","mountPath":"/var/lib/kubelet","readOnly":false,"mountPropagation":null,"subPath":null,"subPathExpr":null}]}],"dnsConfig":null,"hostAliases":null,"hostIPC":false,"hostNetwork":true,"hostPID":false,"initContainers":[{"args":null,"command":["/bin/bash","-c","#/bin/bash\n/host/bin/oc --kubeconfig /var/lib/kubelet/kubeconfig get nodes\n"],"env":null,"image":"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1e747154a68ae161551581454ef86237b20192bd0ebf72a7ea4e4b8a9ab0f242","livenessProbe":null,"name":"verify-api-int-resolvable","imageDetails":{"registry":"quay.io/openshift-release-dev","repository":"ocp-v4.0-art-dev","tag":"sha256:1e747154a68ae161551581454ef86237b20192bd0ebf72a7ea4e4b8a9ab0f242"},"parsedArgs":[{"key":"c","value":"#/bin/bash/host/bin/oc"},{"key":"kubeconfig","value":"/var/lib/kubelet/kubeconfig get nodes"}],"ports":null,"readinessProbe":null,"resources":{"limits":null,"requests":null},"securityContext":null,"volumeMounts":[{"name":"chroot-host","mountPath":"/host","readOnly":false,"mountPropagation":null,"subPath":null,"subPathExpr":null},{"name":"kubeconfigvarlib","mountPath":"/var/lib/kubelet","readOnly":false,"mountPropagation":null,"subPath":null,"subPathExpr":null}]}],"nodeInfo":{"conditions":[{"message":"kubelet has sufficient memory available","reason":"KubeletHasSufficientMemory","status":"False","type":"MemoryPressure"},{"message":"kubelet has no disk pressure","reason":"KubeletHasNoDiskPressure","status":"False","type":"DiskPressure"},{"message":"kubelet has sufficient PID available","reason":"KubeletHasSufficientPID","status":"False","type":"PIDPressure"},{"message":"kubelet is posting ready status","reason":"KubeletReady","status":"True","type":"Ready"}],"labels":[{"key":"beta.kubernetes.io/arch","value":"amd64"},{"key":"beta.kubernetes.io/os","value":"linux"},{"key":"kubernetes.io/arch","value":"amd64"},{"key":"kubernetes.io/hostname","value":"control-1.demoenv.local"},{"key":"kubernetes.io/os","value":"linux"},{"key":"node-role.kubernetes.io/master","value":""},{"key":"node.openshift.io/os_id","value":"rhcos"}]},"nodeName":"control-1.demoenv.local","priority":2000001000,"priorityClassName":"system-node-critical","securityContext":{"fsGroup":null,"runAsGroup":null,"runAsNonRoot":null,"runAsUser":null,"seLinuxOptions":null,"supplementalGroups":null,"sysctls":null,"windowsOptions":null,"seccompProfile":null},"serviceAccount":null,"serviceAccountName":null,"tolerations":[{"key":null,"operator":"Exists","effect":null}],"volumes":[{"hostPath":{"path":"/etc/kubernetes/static-pod-resources/haproxy","type":""},"flexVolume":null,"name":"resource-dir","persistentVolumeClaim":null},{"hostPath":{"path":"/var/lib/kubelet","type":""},"flexVolume":null,"name":"kubeconfigvarlib","persistentVolumeClaim":null},{"hostPath":null,"flexVolume":null,"name":"run-dir","persistentVolumeClaim":null},{"hostPath":{"path":"/etc/haproxy","type":""},"flexVolume":null,"name":"conf-dir","persistentVolumeClaim":null},{"hostPath":{"path":"/","type":""},"flexVolume":null,"name":"chroot-host","persistentVolumeClaim":null}]},"status":{"phase":"Running","podIP":"192.168.10.11"},"networkPolicies":{"ingress":[],"egress":[]},"owner":{"ownerReferences":[{"kind":"Node","uid":"abddf8bc-d0d0-42d8-9b1f-8dbe92ca0cbe","name":"control-1.demoenv.local"}],"rootOwner":{"kind":"Node","uid":"abddf8bc-d0d0-42d8-9b1f-8dbe92ca0cbe","name":"control-1.demoenv.local"}},"tags":[{"key":"app","value":"kni-infra-api-lb"}],"namespace":"openshift-kni-infra","annotations":[{"key":"kubernetes.io/config.hash","value":"0dc8099d0b4812f2ea7ee90ed579f633"},{"key":"kubernetes.io/config.mirror","value":"0dc8099d0b4812f2ea7ee90ed579f633"},{"key":"kubernetes.io/config.seen","value":"2022-12-21T04:50:06.684874422Z"},{"key":"kubernetes.io/config.source","value":"file"},{"key":"openshift.io/scc","value":"privileged"}],"labels":[{"key":"app","value":"kni-infra-api-lb"}],"creationTime":1671598219,"id":"27d86f45-c27a-4c18-b6d3-a9d0ffdeea8d","type":"KubernetesPod","name":"haproxy-control-1.demoenv.local","dome9Id":"11|5af13ab5-72ed-43b6-9544-33c9236d62f0|Pod|27d86f45-c27a-4c18-b6d3-a9d0ffdeea8d","accountNumber":"5af13ab5-72ed-43b6-9544-33c9236d62f0","assetLabels":null,"region":"Global","externalFindings":null},"remediationActions":[],"action":"Detect","additionalFields":[]}