{"status":"Failed","policy":{"name":"Webhooks","description":""},"findingKey":"2mDth5NevThwnWwMupzGCA","findingId":"2mDth5NevThwnWwMupzGCA","cloudGuardAccountId":"51722","origin":"Compliance Engine","bundle":{"name":"CIS Kubernetes Benchmark v1.6.1","description":"Automated Validation of Kubernetes CIS Benchmark v1.6.1 Prescriptive guidance for establishing a secure configuration posture for Kubernetes 1.6.1\nFor additional reference: https://www.cisecurity.org/benchmark/kubernetes/","id":-70},"reportTime":"2023-06-09T11:00:31.243Z","rule":{"name":"Ensure that the seccomp profile is set to docker/default in your pod definitions","ruleId":"D9.K8S.IAM.21","description":"Seccomp (secure computing mode) is used to restrict the set of system calls applications can make, allowing cluster administrators greater control over the security of workloads running in the cluster. Kubernetes disables seccomp profiles by default for historical reasons. You should enable it to ensure that the workloads have restricted actions available within the container.","remediation":"Use security context to enable the docker/default seccomp profile in your pod definitions.\n\n**References**\n1. https://github.com/kubernetes/kubernetes/issues/39845\n2. https://github.com/kubernetes/kubernetes/pull/21790\n3. https://github.com/kubernetes/community/blob/master/contributors/design-proposals/seccomp.md#examples\n4. https://docs.docker.com/engine/security/seccomp/","complianceTags":"5.6.2","logicHash":"wat89y5RGQIVJriupbowlQ","severity":"High"},"account":{"id":"5af13ab5-72ed-43b6-9544-33c9236d62f0","name":"nu-openshift-cluster","vendor":"Kubernetes","dome9CloudAccountId":"5af13ab5-72ed-43b6-9544-33c9236d62f0","organizationalUnitId":"00000000-0000-0000-0000-000000000000","organizationalUnitPath":""},"region":"Global","entity":{"spec":{"affinity":{"nodeAffinity":{"preferredDuringSchedulingIgnoredDuringExecution":null,"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchExpressions":null,"matchFields":[{"key":"metadata.name","operator":"In","values":["control-3.demoenv.local"]}]}]}},"podAffinity":null,"podAntiAffinity":null},"containers":[{"args":null,"command":["/bin/bash","-c","#!/bin/bash\nset -uo pipefail\n\ntrap 'jobs -p | xargs kill || true; wait; exit 0' TERM\n\nOPENSHIFT_MARKER=\"openshift-generated-node-resolver\"\nHOSTS_FILE=\"/etc/hosts\"\nTEMP_FILE=\"/etc/hosts.tmp\"\n\nIFS=', ' read -r -a services <<< \"${SERVICES}\"\n\n# Make a temporary file with the old hosts file's attributes.\ncp -f --attributes-only \"${HOSTS_FILE}\" \"${TEMP_FILE}\"\n\nwhile true; do\n  declare -A svc_ips\n  for svc in \"${services[@]}\"; do\n    # Fetch service IP from cluster dns if present. We make several tries\n    # to do it: IPv4, IPv6, IPv4 over TCP and IPv6 over TCP. The two last ones\n    # are for deployments with Kuryr on older OpenStack (OSP13) - those do not\n    # support UDP loadbalancers and require reaching DNS through TCP.\n    cmds=('dig -t A @\"${NAMESERVER}\" +short \"${svc}.${CLUSTER_DOMAIN}\"|grep -v \"^;\"'\n          'dig -t AAAA @\"${NAMESERVER}\" +short \"${svc}.${CLUSTER_DOMAIN}\"|grep -v \"^;\"'\n          'dig -t A +tcp +retry=0 @\"${NAMESERVER}\" +short \"${svc}.${CLUSTER_DOMAIN}\"|grep -v \"^;\"'\n          'dig -t AAAA +tcp +retry=0 @\"${NAMESERVER}\" +short \"${svc}.${CLUSTER_DOMAIN}\"|grep -v \"^;\"')\n    for i in ${!cmds[*]}\n    do\n      ips=($(eval \"${cmds[i]}\"))\n      if [[ \"$?\" -eq 0 && \"${#ips[@]}\" -ne 0 ]]; then\n        svc_ips[\"${svc}\"]=\"${ips[@]}\"\n        break\n      fi\n    done\n  done\n\n  # Update /etc/hosts only if we get valid service IPs\n  # We will not update /etc/hosts when there is coredns service outage or api unavailability\n  # Stale entries could exist in /etc/hosts if the service is deleted\n  if [[ -n \"${svc_ips[*]-}\" ]]; then\n    # Build a new hosts file from /etc/hosts with our custom entries filtered out\n    grep -v \"# ${OPENSHIFT_MARKER}\" \"${HOSTS_FILE}\" > \"${TEMP_FILE}\"\n\n    # Append resolver entries for services\n    for svc in \"${!svc_ips[@]}\"; do\n      for ip in ${svc_ips[${svc}]}; do\n        echo \"${ip} ${svc} ${svc}.${CLUSTER_DOMAIN} # ${OPENSHIFT_MARKER}\" >> \"${TEMP_FILE}\"\n      done\n    done\n\n    # TODO: Update /etc/hosts atomically to avoid any inconsistent behavior\n    # Replace /etc/hosts with our modified version if needed\n    cmp \"${TEMP_FILE}\" \"${HOSTS_FILE}\" || cp -f \"${TEMP_FILE}\" \"${HOSTS_FILE}\"\n    # TEMP_FILE is not removed to avoid file create/delete and attributes copy churn\n  fi\n  sleep 60 & wait\n  unset svc_ips\ndone\n"],"env":[{"name":"SERVICES","value":"image-registry.openshift-image-registry.svc","valueFrom":null},{"name":"NAMESERVER","value":"172.30.0.10","valueFrom":null},{"name":"CLUSTER_DOMAIN","value":"cluster.local","valueFrom":null}],"image":"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6e8c427bac50a74e9c1a6ca367e2ac0c152bf39484368f5c371738a409cd205d","livenessProbe":null,"name":"dns-node-resolver","imageDetails":{"registry":"quay.io/openshift-release-dev","repository":"ocp-v4.0-art-dev","tag":"sha256:6e8c427bac50a74e9c1a6ca367e2ac0c152bf39484368f5c371738a409cd205d"},"parsedArgs":[{"key":"c","value":"#!/bin/bashset"},{"key":"uo","value":"pipefailtrap 'jobs"},{"key":"p","value":"| xargs kill || true; wait; exit 0' TERMOPENSHIFT_MARKER=\"openshift-generated-node-resolver\"HOSTS_FILE=\"/etc/hosts\"TEMP_FILE=\"/etc/hosts.tmp\"IFS=', ' read"},{"key":"a","value":"services <<< \"${SERVICES}\"# Make a temporary file with the old hosts file's attributes.cp"},{"key":"attributes-only","value":"\"${HOSTS_FILE}\" \"${TEMP_FILE}\"while true; do  declare"},{"key":"A","value":"svc_ips  for svc in \"${services[@]}\"; do    # Fetch service IP from cluster dns if present. We make several tries    # to do it: IPv4, IPv6, IPv4 over TCP and IPv6 over TCP. The two last ones    # are for deployments with Kuryr on older OpenStack (OSP13)"},{"key":"","value":"those do not    # support UDP loadbalancers and require reaching DNS through TCP.    cmds=('dig"},{"key":"t","value":"A @\"${NAMESERVER}\" +short \"${svc}.${CLUSTER_DOMAIN}\"|grep"},{"key":"v","value":"\"^;\"'          'dig"},{"key":"t","value":"AAAA @\"${NAMESERVER}\" +short \"${svc}.${CLUSTER_DOMAIN}\"|grep"},{"key":"v","value":"\"^;\"'          'dig"},{"key":"t","value":"A +tcp +retry=0 @\"${NAMESERVER}\" +short \"${svc}.${CLUSTER_DOMAIN}\"|grep"},{"key":"v","value":"\"^;\"'          'dig"},{"key":"t","value":"AAAA +tcp +retry=0 @\"${NAMESERVER}\" +short \"${svc}.${CLUSTER_DOMAIN}\"|grep"},{"key":"v","value":"\"^;\"')    for i in ${!cmds[*]}    do      ips=($(eval \"${cmds[i]}\"))      if [[ \"$?\""},{"key":"eq","value":"0 && \"${#ips[@]}\""},{"key":"ne","value":"0 ]]; then        svc_ips[\"${svc}\"]=\"${ips[@]}\"        break      fi    done  done  # Update /etc/hosts only if we get valid service IPs  # We will not update /etc/hosts when there is coredns service outage or api unavailability  # Stale entries could exist in /etc/hosts if the service is deleted  if [["},{"key":"n","value":"\"${svc_ips[*]-}\" ]]; then    # Build a new hosts file from /etc/hosts with our custom entries filtered out    grep"},{"key":"v","value":"\"# ${OPENSHIFT_MARKER}\" \"${HOSTS_FILE}\" > \"${TEMP_FILE}\"    # Append resolver entries for services    for svc in \"${!svc_ips[@]}\"; do      for ip in ${svc_ips[${svc}]}; do        echo \"${ip} ${svc} ${svc}.${CLUSTER_DOMAIN} # ${OPENSHIFT_MARKER}\" >> \"${TEMP_FILE}\"      done    done    # TODO: Update /etc/hosts atomically to avoid any inconsistent behavior    # Replace /etc/hosts with our modified version if needed    cmp \"${TEMP_FILE}\" \"${HOSTS_FILE}\" || cp"},{"key":"f","value":"\"${TEMP_FILE}\" \"${HOSTS_FILE}\"    # TEMP_FILE is not removed to avoid file create/delete and attributes copy churn  fi  sleep 60 & wait  unset svc_ipsdone"}],"ports":null,"readinessProbe":null,"resources":{"limits":null,"requests":{"memory":"21Mi","cpu":"5m"}},"securityContext":{"allowPrivilegeEscalation":null,"capabilities":null,"privileged":true,"procMount":null,"readOnlyRootFilesystem":null,"runAsGroup":null,"runAsNonRoot":null,"runAsUser":null,"seLinuxOptions":null,"windowsOptions":null,"seccompProfile":null},"volumeMounts":[{"name":"hosts-file","mountPath":"/etc/hosts","readOnly":false,"mountPropagation":null,"subPath":null,"subPathExpr":null},{"name":"kube-api-access-7qgpx","mountPath":"/var/run/secrets/kubernetes.io/serviceaccount","readOnly":true,"mountPropagation":null,"subPath":null,"subPathExpr":null}]}],"dnsConfig":null,"hostAliases":null,"hostIPC":false,"hostNetwork":true,"hostPID":false,"initContainers":null,"nodeInfo":{"conditions":[{"message":"kubelet has sufficient memory available","reason":"KubeletHasSufficientMemory","status":"False","type":"MemoryPressure"},{"message":"kubelet has no disk pressure","reason":"KubeletHasNoDiskPressure","status":"False","type":"DiskPressure"},{"message":"kubelet has sufficient PID available","reason":"KubeletHasSufficientPID","status":"False","type":"PIDPressure"},{"message":"kubelet is posting ready status","reason":"KubeletReady","status":"True","type":"Ready"}],"labels":[{"key":"beta.kubernetes.io/arch","value":"amd64"},{"key":"beta.kubernetes.io/os","value":"linux"},{"key":"kubernetes.io/arch","value":"amd64"},{"key":"kubernetes.io/hostname","value":"control-3.demoenv.local"},{"key":"kubernetes.io/os","value":"linux"},{"key":"node-role.kubernetes.io/master","value":""},{"key":"node.openshift.io/os_id","value":"rhcos"}]},"nodeName":"control-3.demoenv.local","priority":2000001000,"priorityClassName":"system-node-critical","securityContext":{"fsGroup":null,"runAsGroup":null,"runAsNonRoot":null,"runAsUser":null,"seLinuxOptions":null,"supplementalGroups":null,"sysctls":null,"windowsOptions":null,"seccompProfile":null},"serviceAccount":"node-resolver","serviceAccountName":"node-resolver","tolerations":[{"key":null,"operator":"Exists","effect":null}],"volumes":[{"hostPath":{"path":"/etc/hosts","type":"File"},"flexVolume":null,"name":"hosts-file","persistentVolumeClaim":null},{"hostPath":null,"flexVolume":null,"name":"kube-api-access-7qgpx","persistentVolumeClaim":null}]},"status":{"phase":"Running","podIP":"192.168.10.13"},"networkPolicies":{"ingress":[],"egress":[]},"owner":{"ownerReferences":[{"kind":"DaemonSet","uid":"871cc6c8-674e-41c3-b041-51cb0fb2fead","name":"node-resolver"}],"rootOwner":{"kind":"DaemonSet","uid":"871cc6c8-674e-41c3-b041-51cb0fb2fead","name":"node-resolver"}},"tags":[{"key":"controller-revision-hash","value":"7b5f846559"},{"key":"dns.operator.openshift.io/daemonset-node-resolver","value":""},{"key":"pod-template-generation","value":"2"}],"namespace":"openshift-dns","annotations":[],"labels":[{"key":"controller-revision-hash","value":"7b5f846559"},{"key":"dns.operator.openshift.io/daemonset-node-resolver","value":""},{"key":"pod-template-generation","value":"2"}],"creationTime":1671597232,"id":"4873e854-0ce9-4c26-b98c-63dbaf5c6543","type":"KubernetesPod","name":"node-resolver-7nxv7","dome9Id":"11|5af13ab5-72ed-43b6-9544-33c9236d62f0|Pod|4873e854-0ce9-4c26-b98c-63dbaf5c6543","accountNumber":"5af13ab5-72ed-43b6-9544-33c9236d62f0","assetLabels":null,"region":"Global","externalFindings":null},"remediationActions":[],"action":"Detect","additionalFields":[]}